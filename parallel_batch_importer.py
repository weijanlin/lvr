# -*- coding: utf-8 -*-
"""
ä¸¦è¡Œæ‰¹æ¬¡åŒ¯å…¥ç³»çµ±
ä½¿ç”¨å¤šåŸ·è¡Œç·’å’Œå¤šç¨‹åºä¾†æå‡åŒ¯å…¥é€Ÿåº¦
"""

import os
import glob
import logging
import time
import threading
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from tqdm import tqdm
import pandas as pd
import queue
import multiprocessing as mp

from config import DB_CONFIG, DATABASES, DATA_FOLDERS, BATCH_SIZE, MAX_WORKERS
from enhanced_data_importer import EnhancedDataImporter
from file_type_mapping import FileTypeMapping
from city_code_mapping import CityCodeMapping

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('parallel_batch_import.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ParallelBatchImporter:
    """ä¸¦è¡Œæ‰¹æ¬¡åŒ¯å…¥å™¨"""
    
    def __init__(self, max_workers: int = None, use_processes: bool = False):
        self.max_workers = max_workers or min(MAX_WORKERS, mp.cpu_count())
        self.use_processes = use_processes
        self.file_mapping = FileTypeMapping()
        self.city_mapping = CityCodeMapping()
        self.stats = {
            'total_folders': 0,
            'total_files': 0,
            'successful_files': 0,
            'failed_files': 0,
            'total_records': 0,
            'start_time': None,
            'end_time': None,
            'folder_stats': {},
            'parallel_stats': {
                'threads_used': 0,
                'processes_used': 0,
                'avg_processing_time': 0
            }
        }
        self.lock = threading.Lock()
    
    def scan_all_folders(self) -> Dict[str, List[str]]:
        """æƒææ‰€æœ‰è³‡æ–™å¤¾ä¸­çš„CSVæª”æ¡ˆ"""
        logger.info("ğŸ” æƒææ‰€æœ‰è³‡æ–™å¤¾ä¸­çš„CSVæª”æ¡ˆ")
        
        all_files = {}
        
        for folder in DATA_FOLDERS:
            if os.path.exists(folder):
                csv_files = glob.glob(os.path.join(folder, "*.csv"))
                all_files[folder] = csv_files
                logger.info(f"ğŸ“ {folder}: æ‰¾åˆ° {len(csv_files)} å€‹CSVæª”æ¡ˆ")
            else:
                logger.warning(f"âš ï¸ è³‡æ–™å¤¾ä¸å­˜åœ¨: {folder}")
                all_files[folder] = []
        
        return all_files
    
    def analyze_files(self, all_files: Dict[str, List[str]]) -> Dict:
        """åˆ†ææª”æ¡ˆåˆ†å¸ƒ"""
        logger.info("ğŸ“Š åˆ†ææª”æ¡ˆåˆ†å¸ƒ")
        
        analysis = {
            'file_types': {},
            'city_distribution': {},
            'total_files': 0,
            'files_by_size': {
                'small': [],    # < 1000 rows
                'medium': [],   # 1000-10000 rows
                'large': []     # > 10000 rows
            }
        }
        
        for folder, files in all_files.items():
            for file_path in files:
                filename = os.path.basename(file_path)
                analysis['total_files'] += 1
                
                # åˆ†ææª”æ¡ˆé¡å‹
                file_info = self.file_mapping.get_file_info(filename)
                if file_info:
                    file_type = file_info['description']
                    if file_type not in analysis['file_types']:
                        analysis['file_types'][file_type] = 0
                    analysis['file_types'][file_type] += 1
                
                # åˆ†æç¸£å¸‚åˆ†å¸ƒ
                city_code = self.city_mapping.extract_city_code_from_filename(filename)
                if city_code:
                    city_name = self.city_mapping.get_city_name(city_code)
                    if city_name not in analysis['city_distribution']:
                        analysis['city_distribution'][city_name] = 0
                    analysis['city_distribution'][city_name] += 1
                
                # ä¼°ç®—æª”æ¡ˆå¤§å°ï¼ˆå¿«é€Ÿé è¦½ï¼‰
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        line_count = sum(1 for _ in f)
                    
                    if line_count < 1000:
                        analysis['files_by_size']['small'].append(file_path)
                    elif line_count < 10000:
                        analysis['files_by_size']['medium'].append(file_path)
                    else:
                        analysis['files_by_size']['large'].append(file_path)
                except:
                    analysis['files_by_size']['small'].append(file_path)
        
        return analysis
    
    def import_single_file_worker(self, file_path: str, folder: str) -> Dict:
        """å–®ä¸€æª”æ¡ˆåŒ¯å…¥å·¥ä½œå‡½æ•¸"""
        filename = os.path.basename(file_path)
        start_time = time.time()
        
        try:
            # å»ºç«‹ç¨ç«‹çš„åŒ¯å…¥å™¨å¯¦ä¾‹
            importer = EnhancedDataImporter()
            success = importer.import_single_file(file_path, folder)
            
            processing_time = time.time() - start_time
            
            return {
                'filename': filename,
                'file_path': file_path,
                'folder': folder,
                'success': success,
                'processing_time': processing_time,
                'error': None
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            return {
                'filename': filename,
                'file_path': file_path,
                'folder': folder,
                'success': False,
                'processing_time': processing_time,
                'error': str(e)
            }
    
    def import_folder_parallel(self, folder: str, files: List[str]) -> Dict:
        """ä¸¦è¡ŒåŒ¯å…¥å–®ä¸€è³‡æ–™å¤¾çš„æ‰€æœ‰æª”æ¡ˆ"""
        logger.info(f"ğŸ“‚ é–‹å§‹ä¸¦è¡ŒåŒ¯å…¥è³‡æ–™å¤¾: {folder} (ä½¿ç”¨ {self.max_workers} å€‹å·¥ä½œåŸ·è¡Œç·’)")
        
        folder_stats = {
            'folder': folder,
            'total_files': len(files),
            'successful_files': 0,
            'failed_files': 0,
            'total_records': 0,
            'file_results': {},
            'errors': [],
            'processing_times': []
        }
        
        # ä½¿ç”¨åŸ·è¡Œç·’æ± é€²è¡Œä¸¦è¡Œè™•ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_file = {
                executor.submit(self.import_single_file_worker, file_path, folder): file_path
                for file_path in files
            }
            
            # ä½¿ç”¨é€²åº¦æ¢è¿½è¹¤é€²åº¦
            with tqdm(total=len(files), desc=f"ä¸¦è¡ŒåŒ¯å…¥ {folder}", unit="æª”æ¡ˆ") as pbar:
                for future in as_completed(future_to_file):
                    result = future.result()
                    filename = result['filename']
                    
                    # æ›´æ–°çµ±è¨ˆ
                    with self.lock:
                        if result['success']:
                            folder_stats['successful_files'] += 1
                            folder_stats['file_results'][filename] = {
                                'status': 'success',
                                'processing_time': result['processing_time']
                            }
                            logger.info(f"âœ… {filename} åŒ¯å…¥æˆåŠŸ ({result['processing_time']:.2f}s)")
                        else:
                            folder_stats['failed_files'] += 1
                            folder_stats['file_results'][filename] = {
                                'status': 'failed',
                                'error': result['error'],
                                'processing_time': result['processing_time']
                            }
                            folder_stats['errors'].append(f"{filename}: {result['error']}")
                            logger.error(f"âŒ {filename} åŒ¯å…¥å¤±æ•—: {result['error']}")
                        
                        folder_stats['processing_times'].append(result['processing_time'])
                    
                    pbar.set_postfix({
                        'æˆåŠŸ': folder_stats['successful_files'],
                        'å¤±æ•—': folder_stats['failed_files'],
                        'å¹³å‡æ™‚é–“': f"{sum(folder_stats['processing_times'])/len(folder_stats['processing_times']):.2f}s"
                    })
                    pbar.update(1)
        
        # è¨ˆç®—å¹³å‡è™•ç†æ™‚é–“
        if folder_stats['processing_times']:
            folder_stats['avg_processing_time'] = sum(folder_stats['processing_times']) / len(folder_stats['processing_times'])
        
        logger.info(f"ğŸ“‚ å®Œæˆä¸¦è¡ŒåŒ¯å…¥è³‡æ–™å¤¾: {folder} - æˆåŠŸ: {folder_stats['successful_files']}, å¤±æ•—: {folder_stats['failed_files']}, å¹³å‡æ™‚é–“: {folder_stats['avg_processing_time']:.2f}s")
        return folder_stats
    
    def import_all_folders_parallel(self, dry_run: bool = False) -> Dict:
        """ä¸¦è¡ŒåŒ¯å…¥æ‰€æœ‰è³‡æ–™å¤¾"""
        logger.info(f"ğŸš€ é–‹å§‹ä¸¦è¡Œæ‰¹æ¬¡åŒ¯å…¥æ‰€æœ‰è³‡æ–™å¤¾ (ä½¿ç”¨ {self.max_workers} å€‹å·¥ä½œåŸ·è¡Œç·’)")
        
        self.stats['start_time'] = datetime.now()
        
        # æƒææ‰€æœ‰æª”æ¡ˆ
        all_files = self.scan_all_folders()
        
        # åˆ†ææª”æ¡ˆåˆ†å¸ƒ
        analysis = self.analyze_files(all_files)
        
        logger.info("ğŸ“Š æª”æ¡ˆåˆ†å¸ƒåˆ†æ:")
        logger.info(f"  ç¸½æª”æ¡ˆæ•¸: {analysis['total_files']}")
        logger.info(f"  å°æª”æ¡ˆ (<1000è¡Œ): {len(analysis['files_by_size']['small'])}")
        logger.info(f"  ä¸­æª”æ¡ˆ (1000-10000è¡Œ): {len(analysis['files_by_size']['medium'])}")
        logger.info(f"  å¤§æª”æ¡ˆ (>10000è¡Œ): {len(analysis['files_by_size']['large'])}")
        logger.info("  æª”æ¡ˆé¡å‹åˆ†å¸ƒ:")
        for file_type, count in analysis['file_types'].items():
            logger.info(f"    {file_type}: {count} å€‹æª”æ¡ˆ")
        logger.info("  ç¸£å¸‚åˆ†å¸ƒ:")
        for city, count in analysis['city_distribution'].items():
            logger.info(f"    {city}: {count} å€‹æª”æ¡ˆ")
        
        if dry_run:
            logger.info("ğŸ” ä¹¾è·‘æ¨¡å¼ - ä¸åŸ·è¡Œå¯¦éš›åŒ¯å…¥")
            return {
                'analysis': analysis,
                'files': all_files,
                'dry_run': True
            }
        
        # ä¸¦è¡ŒåŒ¯å…¥æ¯å€‹è³‡æ–™å¤¾
        for folder, files in all_files.items():
            if files:
                folder_stats = self.import_folder_parallel(folder, files)
                self.stats['folder_stats'][folder] = folder_stats
                self.stats['total_files'] += folder_stats['total_files']
                self.stats['successful_files'] += folder_stats['successful_files']
                self.stats['failed_files'] += folder_stats['failed_files']
                self.stats['total_records'] += folder_stats['total_records']
            else:
                logger.warning(f"âš ï¸ è³‡æ–™å¤¾ {folder} æ²’æœ‰CSVæª”æ¡ˆ")
        
        self.stats['end_time'] = datetime.now()
        self.stats['total_folders'] = len([f for f in all_files.values() if f])
        self.stats['parallel_stats']['threads_used'] = self.max_workers
        
        # è¨ˆç®—å¹³å‡è™•ç†æ™‚é–“
        all_processing_times = []
        for folder_stats in self.stats['folder_stats'].values():
            all_processing_times.extend(folder_stats['processing_times'])
        
        if all_processing_times:
            self.stats['parallel_stats']['avg_processing_time'] = sum(all_processing_times) / len(all_processing_times)
        
        # ç”ŸæˆåŒ¯å…¥å ±å‘Š
        self.generate_parallel_import_report()
        
        return self.stats
    
    def generate_parallel_import_report(self):
        """ç”Ÿæˆä¸¦è¡ŒåŒ¯å…¥å ±å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆä¸¦è¡ŒåŒ¯å…¥å ±å‘Š")
        
        duration = self.stats['end_time'] - self.stats['start_time']
        success_rate = (self.stats['successful_files'] / self.stats['total_files'] * 100) if self.stats['total_files'] > 0 else 0
        
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                           ä¸¦è¡Œæ‰¹æ¬¡åŒ¯å…¥å ±å‘Š                                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ åŒ¯å…¥æ™‚é–“: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')} - {self.stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}                    â•‘
â•‘ ç¸½è€—æ™‚: {duration}                                                           â•‘
â•‘                                                                              â•‘
â•‘ ä¸¦è¡Œè™•ç†çµ±è¨ˆ:                                                                â•‘
â•‘   ä½¿ç”¨åŸ·è¡Œç·’æ•¸: {self.stats['parallel_stats']['threads_used']}                                                      â•‘
â•‘   å¹³å‡è™•ç†æ™‚é–“: {self.stats['parallel_stats']['avg_processing_time']:.2f}ç§’/æª”æ¡ˆ                                        â•‘
â•‘                                                                              â•‘
â•‘ è³‡æ–™å¤¾çµ±è¨ˆ:                                                                  â•‘
â•‘   ç¸½è³‡æ–™å¤¾æ•¸: {self.stats['total_folders']}                                                      â•‘
â•‘   ç¸½æª”æ¡ˆæ•¸: {self.stats['total_files']}                                                        â•‘
â•‘   æˆåŠŸæª”æ¡ˆæ•¸: {self.stats['successful_files']}                                                    â•‘
â•‘   å¤±æ•—æª”æ¡ˆæ•¸: {self.stats['failed_files']}                                                      â•‘
â•‘   æˆåŠŸç‡: {success_rate:.1f}%                                                      â•‘
â•‘   ç¸½è¨˜éŒ„æ•¸: {self.stats['total_records']:,}                                                      â•‘
â•‘                                                                              â•‘
â•‘ å„è³‡æ–™å¤¾è©³ç´°çµ±è¨ˆ:                                                            â•‘
"""
        
        for folder, stats in self.stats['folder_stats'].items():
            folder_success_rate = (stats['successful_files'] / stats['total_files'] * 100) if stats['total_files'] > 0 else 0
            report += f"â•‘   {folder}: {stats['successful_files']}/{stats['total_files']} ({folder_success_rate:.1f}%) - å¹³å‡: {stats['avg_processing_time']:.2f}s/æª”æ¡ˆ\n"
        
        report += "â•‘                                                                              â•‘\n"
        
        if self.stats['failed_files'] > 0:
            report += "â•‘ å¤±æ•—æª”æ¡ˆåˆ—è¡¨:                                                              â•‘\n"
            for folder, stats in self.stats['folder_stats'].items():
                if stats['errors']:
                    report += f"â•‘   {folder}:\n"
                    for error in stats['errors'][:3]:  # åªé¡¯ç¤ºå‰3å€‹éŒ¯èª¤
                        report += f"â•‘     - {error}\n"
                    if len(stats['errors']) > 3:
                        report += f"â•‘     ... é‚„æœ‰ {len(stats['errors']) - 3} å€‹éŒ¯èª¤\n"
        
        report += "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        logger.info(report)
        
        # ä¿å­˜å ±å‘Šåˆ°æª”æ¡ˆ
        with open('parallel_batch_import_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info("ğŸ“„ ä¸¦è¡ŒåŒ¯å…¥å ±å‘Šå·²ä¿å­˜åˆ° parallel_batch_import_report.txt")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ ä¸¦è¡Œæ‰¹æ¬¡åŒ¯å…¥ç³»çµ±")
    print("=" * 80)
    
    # è©¢å•ä¸¦è¡Œè¨­å®š
    print("ä¸¦è¡Œè™•ç†è¨­å®š:")
    print("1. è‡ªå‹•è¨­å®š (å»ºè­°)")
    print("2. æ‰‹å‹•è¨­å®šåŸ·è¡Œç·’æ•¸")
    
    choice = input("è«‹é¸æ“‡ (1/2): ").strip()
    
    if choice == "2":
        try:
            max_workers = int(input("è«‹è¼¸å…¥åŸ·è¡Œç·’æ•¸ (å»ºè­° 2-8): ").strip())
        except ValueError:
            max_workers = 4
            print(f"ä½¿ç”¨é è¨­å€¼: {max_workers}")
    else:
        max_workers = min(MAX_WORKERS, mp.cpu_count())
        print(f"è‡ªå‹•è¨­å®šåŸ·è¡Œç·’æ•¸: {max_workers}")
    
    importer = ParallelBatchImporter(max_workers=max_workers)
    
    # è©¢å•æ˜¯å¦åŸ·è¡Œä¹¾è·‘
    print("\né¸æ“‡åŸ·è¡Œæ¨¡å¼:")
    print("1. ä¹¾è·‘æ¨¡å¼ (åªåˆ†ææª”æ¡ˆï¼Œä¸åŸ·è¡ŒåŒ¯å…¥)")
    print("2. å¯¦éš›åŒ¯å…¥æ¨¡å¼")
    
    choice = input("è«‹é¸æ“‡ (1/2): ").strip()
    
    if choice == "1":
        result = importer.import_all_folders_parallel(dry_run=True)
        print("\nğŸ” ä¹¾è·‘æ¨¡å¼å®Œæˆ")
        print(f"ç¸½æª”æ¡ˆæ•¸: {result['analysis']['total_files']}")
        print("æª”æ¡ˆå¤§å°åˆ†å¸ƒ:")
        for size, files in result['analysis']['files_by_size'].items():
            print(f"  {size}: {len(files)} å€‹æª”æ¡ˆ")
    else:
        print(f"\nâš ï¸ å³å°‡é–‹å§‹ä¸¦è¡ŒåŒ¯å…¥ï¼Œä½¿ç”¨ {max_workers} å€‹åŸ·è¡Œç·’...")
        confirm = input("ç¢ºå®šè¦ç¹¼çºŒå—? (y/N): ").strip().lower()
        
        if confirm == 'y':
            result = importer.import_all_folders_parallel(dry_run=False)
            print("\nâœ… ä¸¦è¡Œæ‰¹æ¬¡åŒ¯å…¥å®Œæˆ!")
            print(f"æˆåŠŸ: {result['successful_files']}/{result['total_files']} æª”æ¡ˆ")
            print(f"æˆåŠŸç‡: {(result['successful_files'] / result['total_files'] * 100):.1f}%")
            print(f"å¹³å‡è™•ç†æ™‚é–“: {result['parallel_stats']['avg_processing_time']:.2f}ç§’/æª”æ¡ˆ")
            print(f"ç¸½è¨˜éŒ„æ•¸: {result['total_records']:,}")
        else:
            print("âŒ åŒ¯å…¥å·²å–æ¶ˆ")

if __name__ == "__main__":
    main()




