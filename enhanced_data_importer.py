# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆè³‡æ–™åŒ¯å…¥å™¨
æ•´åˆç¸£å¸‚ä»£ç¢¼å°æ‡‰åŠŸèƒ½ï¼Œæ”¯æ´æ‰€æœ‰æª”æ¡ˆé¡å‹çš„åŒ¯å…¥
"""

import pandas as pd
import pyodbc
import logging
import os
import glob
from typing import Dict, List, Optional, Tuple
from config import DB_CONFIG, BATCH_SIZE
from file_type_mapping import FileTypeMapping, DataType, FileType
from city_code_mapping import CityCodeMapping

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_import.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedDataImporter:
    """å¢å¼·ç‰ˆè³‡æ–™åŒ¯å…¥å™¨ï¼ˆå«ç¸£å¸‚ä»£ç¢¼ï¼‰"""
    
    def __init__(self):
        self.connection_string = self._build_connection_string()
        self.file_mapping = FileTypeMapping()
        self.city_mapping = CityCodeMapping()
        
    def _build_connection_string(self) -> str:
        """å»ºç«‹é€£ç·šå­—ä¸²"""
        return (
            f"DRIVER={{{DB_CONFIG['driver']}}};"
            f"SERVER={DB_CONFIG['server']};"
            f"UID={DB_CONFIG['username']};"
            f"PWD={DB_CONFIG['password']};"
            f"Trusted_Connection={DB_CONFIG['trusted_connection']};"
            f"Encrypt={DB_CONFIG['encrypt']};"
        )
    
    def read_csv_file(self, file_path: str) -> Optional[pd.DataFrame]:
        """è®€å– CSV æª”æ¡ˆ"""
        try:
            # å˜—è©¦ä¸åŒçš„ç·¨ç¢¼
            encodings = ['utf-8', 'big5', 'cp950', 'gbk']
            
            for encoding in encodings:
                try:
                    # è®€å– CSVï¼Œè·³éç¬¬äºŒè¡Œ (æ¬„ä½åç¨±)
                    df = pd.read_csv(file_path, encoding=encoding, skiprows=[1])
                    logger.info(f"âœ… æˆåŠŸè®€å– {file_path} (ç·¨ç¢¼: {encoding})")
                    return df
                except UnicodeDecodeError:
                    continue
            
            logger.error(f"âŒ ç„¡æ³•è®€å– {file_path}ï¼Œæ‰€æœ‰ç·¨ç¢¼éƒ½å¤±æ•—")
            return None
            
        except Exception as e:
            logger.error(f"âŒ è®€å– {file_path} å¤±æ•—: {str(e)}")
            return None
    
    def clean_data(self, df: pd.DataFrame, file_type: FileType) -> pd.DataFrame:
        """æ¸…ç†è³‡æ–™"""
        try:
            # ç§»é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
            df = df.dropna(how='all')
            
            # æ ¹æ“šæª”æ¡ˆé¡å‹å®šç¾©æ•¸å€¼æ¬„ä½
            numeric_columns = self._get_numeric_columns(file_type)
            
            # è™•ç†æ•¸å€¼æ¬„ä½
            for col in numeric_columns:
                if col in df.columns:
                    # å…ˆè½‰æ›ç‚ºå­—ä¸²ï¼Œç„¶å¾Œæ¸…ç†
                    df[col] = df[col].astype(str)
                    # ç§»é™¤éæ•¸å€¼å­—ç¬¦ï¼ˆä¿ç•™å°æ•¸é»å’Œè² è™Ÿï¼‰
                    df[col] = df[col].str.replace(r'[^\d.-]', '', regex=True)
                    # è™•ç†ç©ºå­—ä¸²å’Œç„¡æ•ˆå€¼
                    df[col] = df[col].replace(['', 'nan', 'None', 'null'], None)
                    # è½‰æ›ç‚ºæ•¸å€¼ï¼Œç„¡æ³•è½‰æ›çš„è¨­ç‚º None
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                    # å°‡ NaN è½‰æ›ç‚º None
                    df[col] = df[col].where(pd.notnull(df[col]), None)
            
            # è™•ç†å­—ä¸²æ¬„ä½çš„ç©ºå€¼
            string_columns = df.select_dtypes(include=['object']).columns
            for col in string_columns:
                df[col] = df[col].fillna('')
                # æ¸…ç†å­—ä¸²ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                df[col] = df[col].astype(str).str.replace('\r\n', ' ').str.replace('\n', ' ').str.strip()
                # è™•ç† 'nan' å­—ä¸²
                df[col] = df[col].replace('nan', '')
            
            logger.info(f"âœ… è³‡æ–™æ¸…ç†å®Œæˆï¼Œå‰©é¤˜ {len(df)} è¡Œ")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™æ¸…ç†å¤±æ•—: {str(e)}")
            return df
    
    def _get_numeric_columns(self, file_type: FileType) -> List[str]:
        """æ ¹æ“šæª”æ¡ˆé¡å‹å–å¾—æ•¸å€¼æ¬„ä½åˆ—è¡¨"""
        if file_type == FileType.MAIN:
            return [
                'åœŸåœ°ç§»è½‰ç¸½é¢ç©å¹³æ–¹å…¬å°º', 'äº¤æ˜“ç­†æ£Ÿæ•¸', 'ç¸½æ¨“å±¤æ•¸', 'å»ºç‰©ç§»è½‰ç¸½é¢ç©å¹³æ–¹å…¬å°º',
                '[å»ºç‰©ç¾æ³æ ¼å±€-æˆ¿]', '[å»ºç‰©ç¾æ³æ ¼å±€-å»³]', '[å»ºç‰©ç¾æ³æ ¼å±€-è¡›]', 'ç¸½åƒ¹å…ƒ', 'å–®åƒ¹å…ƒå¹³æ–¹å…¬å°º',
                'è»Šä½ç§»è½‰ç¸½é¢ç©å¹³æ–¹å…¬å°º', 'è»Šä½ç¸½åƒ¹å…ƒ', 'ä¸»å»ºç‰©é¢ç©', 'é™„å±¬å»ºç‰©é¢ç©', 'é™½å°é¢ç©',
                'åœŸåœ°é¢ç©å¹³æ–¹å…¬å°º', 'å»ºç‰©ç¸½é¢ç©å¹³æ–¹å…¬å°º', 'è»Šä½é¢ç©å¹³æ–¹å…¬å°º', 'è»Šä½ç¸½é¡å…ƒ', 'ç¸½é¡å…ƒ',
                'ç§Ÿè³ƒç­†æ£Ÿæ•¸', 'å±‹é½¡', 'å»ºç‰©ç§»è½‰é¢ç©å¹³æ–¹å…¬å°º', 'æ¬Šåˆ©äººæŒåˆ†åˆ†æ¯', 'æ¬Šåˆ©äººæŒåˆ†åˆ†å­',
                'è»Šä½åƒ¹æ ¼', 'è»Šä½é¢ç©å¹³æ–¹å…¬å°º'
            ]
        elif file_type == FileType.BUILD:
            return [
                'å±‹é½¡', 'å»ºç‰©ç§»è½‰é¢ç©å¹³æ–¹å…¬å°º', 'ç¸½å±¤æ•¸'
            ]
        elif file_type == FileType.LAND:
            return [
                'åœŸåœ°ç§»è½‰é¢ç©å¹³æ–¹å…¬å°º', 'æ¬Šåˆ©äººæŒåˆ†åˆ†æ¯', 'æ¬Šåˆ©äººæŒåˆ†åˆ†å­'
            ]
        elif file_type == FileType.PARK:
            return [
                'è»Šä½åƒ¹æ ¼', 'è»Šä½é¢ç©å¹³æ–¹å…¬å°º'
            ]
        else:
            return []
    
    def create_insert_sql(self, table_name: str, columns: List[str]) -> str:
        """å»ºç«‹ INSERT SQL èªå¥"""
        # åŠ å…¥é¡å¤–æ¬„ä½ï¼ˆç¸£å¸‚ä»£ç¢¼ã€ç¸£å¸‚åç¨±ã€source_fileã€quarterï¼‰
        all_columns = ['ç¸£å¸‚ä»£ç¢¼', 'ç¸£å¸‚åç¨±'] + columns + ['source_file', 'quarter']
        placeholders = ', '.join(['?' for _ in all_columns])
        column_names = ', '.join([f'[{col}]' if '-' in col else col for col in all_columns])
        
        return f"INSERT INTO [{table_name}] ({column_names}) VALUES ({placeholders})"
    
    def insert_data_batch(self, database_name: str, table_name: str, df: pd.DataFrame,
                         source_file: str, quarter: str, city_code: str, city_name: str) -> bool:
        """æ‰¹æ¬¡æ’å…¥è³‡æ–™ï¼ˆå«ç¸£å¸‚ä»£ç¢¼ï¼‰"""
        try:
            # é€£æ¥åˆ°æŒ‡å®šè³‡æ–™åº«
            conn_str = self.connection_string + f"Database={database_name};"
            conn = pyodbc.connect(conn_str)
            cursor = conn.cursor()
            
            # æº–å‚™è³‡æ–™
            columns = list(df.columns)
            insert_sql = self.create_insert_sql(table_name, columns)
            
            # æ‰¹æ¬¡è™•ç†
            total_rows = len(df)
            success_count = 0
            
            for i in range(0, total_rows, BATCH_SIZE):
                batch_df = df.iloc[i:i+BATCH_SIZE]
                batch_data = []
                
                for _, row in batch_df.iterrows():
                    # æº–å‚™è³‡æ–™è¡Œï¼Œè™•ç†è³‡æ–™é¡å‹
                    row_data = []
                    
                    # åŠ å…¥ç¸£å¸‚ä»£ç¢¼å’Œç¸£å¸‚åç¨±
                    row_data.extend([city_code, city_name])
                    
                    for value in row.values:
                        if pd.isna(value) or value is None:
                            row_data.append(None)
                        elif isinstance(value, (int, float)):
                            # ç¢ºä¿æ•¸å€¼åœ¨åˆç†ç¯„åœå…§
                            if isinstance(value, float) and (value > 1e15 or value < -1e15):
                                row_data.append(None)
                            else:
                                row_data.append(value)
                        else:
                            # å­—ä¸²è³‡æ–™
                            str_value = str(value).strip()
                            if str_value in ['', 'nan', 'None', 'null']:
                                row_data.append(None)
                            else:
                                row_data.append(str_value)
                    
                    # åŠ å…¥é¡å¤–æ¬„ä½
                    row_data.extend([source_file, quarter])
                    batch_data.append(row_data)
                
                # åŸ·è¡Œæ‰¹æ¬¡æ’å…¥
                cursor.executemany(insert_sql, batch_data)
                success_count += len(batch_data)
                
                # é¡¯ç¤ºé€²åº¦
                progress = min(i + BATCH_SIZE, total_rows)
                logger.info(f"ğŸ“Š é€²åº¦: {progress}/{total_rows} è¡Œå·²è™•ç†")
            
            conn.commit()
            conn.close()
            
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {success_count} è¡Œåˆ° {database_name}.{table_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ’å…¥è³‡æ–™åˆ° {database_name}.{table_name} å¤±æ•—: {str(e)}")
            return False
    
    def import_single_file(self, file_path: str, quarter: str) -> bool:
        """åŒ¯å…¥å–®ä¸€æª”æ¡ˆï¼ˆå«ç¸£å¸‚ä»£ç¢¼ï¼‰"""
        try:
            filename = os.path.basename(file_path)
            logger.info(f"ğŸ”„ é–‹å§‹åŒ¯å…¥æª”æ¡ˆ: {filename}")
            
            # å–å¾—æª”æ¡ˆé¡å‹è³‡è¨Š
            file_info = self.file_mapping.get_file_info(filename)
            if not file_info:
                logger.error(f"âŒ ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹: {filename}")
                return False
            
            # å–å¾—ç¸£å¸‚è³‡è¨Š
            city_info = self.city_mapping.get_city_info_from_filename(filename)
            if not city_info:
                logger.error(f"âŒ ç„¡æ³•è­˜åˆ¥ç¸£å¸‚ä»£ç¢¼: {filename}")
                return False
            
            logger.info(f"ğŸ“‹ æª”æ¡ˆè³‡è¨Š: {file_info['description']} â†’ {file_info['database_name']}.{file_info['table_name']}")
            logger.info(f"ğŸ™ï¸ ç¸£å¸‚è³‡è¨Š: {city_info['city_code']} ({city_info['city_name']})")
            
            # è®€å–CSVæª”æ¡ˆ
            df = self.read_csv_file(file_path)
            if df is None or df.empty:
                logger.error(f"âŒ æª”æ¡ˆç‚ºç©ºæˆ–è®€å–å¤±æ•—: {filename}")
                return False
            
            # æ¸…ç†è³‡æ–™
            df = self.clean_data(df, file_info['file_type'])
            if df.empty:
                logger.error(f"âŒ æ¸…ç†å¾Œè³‡æ–™ç‚ºç©º: {filename}")
                return False
            
            # æ’å…¥è³‡æ–™
            success = self.insert_data_batch(
                file_info['database_name'],
                file_info['table_name'],
                df,
                filename,
                quarter,
                city_info['city_code'],
                city_info['city_name']
            )
            
            if success:
                logger.info(f"âœ… æª”æ¡ˆåŒ¯å…¥æˆåŠŸ: {filename}")
                return True
            else:
                logger.error(f"âŒ æª”æ¡ˆåŒ¯å…¥å¤±æ•—: {filename}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ åŒ¯å…¥æª”æ¡ˆå¤±æ•— {file_path}: {str(e)}")
            return False
    
    def import_single_folder(self, folder_name: str) -> Dict[str, int]:
        """åŒ¯å…¥å–®ä¸€è³‡æ–™å¤¾çš„æ‰€æœ‰æª”æ¡ˆï¼ˆå«ç¸£å¸‚ä»£ç¢¼ï¼‰"""
        logger.info(f"ğŸ”„ é–‹å§‹åŒ¯å…¥è³‡æ–™å¤¾: {folder_name}")
        print(f"ğŸ”„ é–‹å§‹åŒ¯å…¥è³‡æ–™å¤¾: {folder_name}")
        
        if not os.path.exists(folder_name):
            logger.error(f"âŒ è³‡æ–™å¤¾ä¸å­˜åœ¨: {folder_name}")
            return {'success': 0, 'failed': 0, 'total': 0}
        
        # å–å¾—æ‰€æœ‰CSVæª”æ¡ˆ
        csv_files = glob.glob(os.path.join(folder_name, "*.csv"))
        
        if not csv_files:
            logger.error(f"âŒ è³‡æ–™å¤¾ä¸­æ²’æœ‰CSVæª”æ¡ˆ: {folder_name}")
            return {'success': 0, 'failed': 0, 'total': 0}
        
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} å€‹CSVæª”æ¡ˆ")
        print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} å€‹CSVæª”æ¡ˆ")
        
        success_count = 0
        failed_count = 0
        
        for file_path in csv_files:
            filename = os.path.basename(file_path)
            print(f"\nğŸ“„ è™•ç†æª”æ¡ˆ: {filename}")
            
            if self.import_single_file(file_path, folder_name):
                success_count += 1
                print(f"âœ… {filename} åŒ¯å…¥æˆåŠŸ")
            else:
                failed_count += 1
                print(f"âŒ {filename} åŒ¯å…¥å¤±æ•—")
        
        result = {
            'success': success_count,
            'failed': failed_count,
            'total': len(csv_files)
        }
        
        logger.info(f"ğŸ“Š è³‡æ–™å¤¾åŒ¯å…¥å®Œæˆ: {folder_name}")
        logger.info(f"   æˆåŠŸ: {success_count}/{len(csv_files)}")
        logger.info(f"   å¤±æ•—: {failed_count}/{len(csv_files)}")
        
        print(f"\nğŸ“Š è³‡æ–™å¤¾åŒ¯å…¥å®Œæˆ: {folder_name}")
        print(f"   æˆåŠŸ: {success_count}/{len(csv_files)}")
        print(f"   å¤±æ•—: {failed_count}/{len(csv_files)}")
        
        return result
    
    def get_folder_statistics(self, folder_name: str) -> Dict:
        """å–å¾—è³‡æ–™å¤¾çµ±è¨ˆè³‡è¨Šï¼ˆå«ç¸£å¸‚ä»£ç¢¼ï¼‰"""
        if not os.path.exists(folder_name):
            return {'error': 'è³‡æ–™å¤¾ä¸å­˜åœ¨'}
        
        csv_files = glob.glob(os.path.join(folder_name, "*.csv"))
        file_stats = {}
        
        for file_path in csv_files:
            filename = os.path.basename(file_path)
            file_info = self.file_mapping.get_file_info(filename)
            city_info = self.city_mapping.get_city_info_from_filename(filename)
            
            if file_info and city_info:
                file_type = f"{file_info['data_type'].value}_{file_info['file_type'].value}"
                city_key = f"{city_info['city_code']}_{city_info['city_name']}"
                
                if file_type not in file_stats:
                    file_stats[file_type] = {}
                
                if city_key not in file_stats[file_type]:
                    file_stats[file_type][city_key] = []
                
                # è®€å–æª”æ¡ˆè¡Œæ•¸
                try:
                    df = pd.read_csv(file_path, encoding='utf-8', skiprows=[1])
                    file_stats[file_type][city_key].append({
                        'filename': filename,
                        'rows': len(df),
                        'columns': len(df.columns)
                    })
                except:
                    file_stats[file_type][city_key].append({
                        'filename': filename,
                        'rows': 0,
                        'columns': 0
                    })
        
        return file_stats

def test_enhanced_importer():
    """æ¸¬è©¦å¢å¼·ç‰ˆåŒ¯å…¥å™¨"""
    print("ğŸ§ª æ¸¬è©¦å¢å¼·ç‰ˆè³‡æ–™åŒ¯å…¥å™¨ï¼ˆå«ç¸£å¸‚ä»£ç¢¼ï¼‰")
    print("=" * 80)
    
    importer = EnhancedDataImporter()
    
    # æ¸¬è©¦è³‡æ–™å¤¾çµ±è¨ˆ
    print("\nğŸ“Š 113Q1 è³‡æ–™å¤¾çµ±è¨ˆï¼ˆå«ç¸£å¸‚ä»£ç¢¼ï¼‰:")
    stats = importer.get_folder_statistics('113Q1')
    
    if 'error' in stats:
        print(f"âŒ {stats['error']}")
        return
    
    for file_type, cities in stats.items():
        print(f"\nğŸ”¸ {file_type}:")
        total_rows = 0
        for city_key, files in cities.items():
            city_code, city_name = city_key.split('_', 1)
            print(f"   ğŸ“ {city_code} ({city_name}):")
            for file_info in files:
                print(f"     {file_info['filename']:<30} - {file_info['rows']:>6} è¡Œ, {file_info['columns']:>2} æ¬„ä½")
                total_rows += file_info['rows']
        print(f"   ç¸½è¨ˆ: {total_rows} è¡Œ")
    
    # æ¸¬è©¦å–®ä¸€æª”æ¡ˆåŒ¯å…¥
    print(f"\nğŸ”„ æ¸¬è©¦å–®ä¸€æª”æ¡ˆåŒ¯å…¥ï¼ˆå«ç¸£å¸‚ä»£ç¢¼ï¼‰...")
    test_file = "113Q1/a_lvr_land_b.csv"  # é å”®å±‹æª”æ¡ˆï¼ˆè‡ºåŒ—å¸‚ï¼‰
    
    if os.path.exists(test_file):
        success = importer.import_single_file(test_file, "113Q1")
        if success:
            print(f"âœ… é å”®å±‹æª”æ¡ˆåŒ¯å…¥æ¸¬è©¦æˆåŠŸï¼ˆå«ç¸£å¸‚ä»£ç¢¼ï¼‰")
        else:
            print(f"âŒ é å”®å±‹æª”æ¡ˆåŒ¯å…¥æ¸¬è©¦å¤±æ•—")
    else:
        print(f"âŒ æ¸¬è©¦æª”æ¡ˆä¸å­˜åœ¨: {test_file}")

if __name__ == "__main__":
    test_enhanced_importer()

