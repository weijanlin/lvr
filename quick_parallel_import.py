# -*- coding: utf-8 -*-
"""
å¿«é€Ÿä¸¦è¡ŒåŒ¯å…¥è…³æœ¬
ç”¨æ–¼å¿«é€Ÿä¸¦è¡ŒåŒ¯å…¥æ‰€æœ‰è³‡æ–™å¤¾çš„CSVæª”æ¡ˆ
"""

import os
import glob
import logging
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import threading

from config import DATA_FOLDERS, MAX_WORKERS
from enhanced_data_importer import EnhancedDataImporter

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('quick_parallel_import.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def import_single_file_worker(file_path: str, folder: str) -> dict:
    """å–®ä¸€æª”æ¡ˆåŒ¯å…¥å·¥ä½œå‡½æ•¸"""
    filename = os.path.basename(file_path)
    start_time = time.time()
    
    try:
        # å»ºç«‹ç¨ç«‹çš„åŒ¯å…¥å™¨å¯¦ä¾‹
        importer = EnhancedDataImporter()
        success = importer.import_single_file(file_path, folder)
        
        processing_time = time.time() - start_time
        
        return {
            'filename': filename,
            'file_path': file_path,
            'folder': folder,
            'success': success,
            'processing_time': processing_time,
            'error': None
        }
        
    except Exception as e:
        processing_time = time.time() - start_time
        return {
            'filename': filename,
            'file_path': file_path,
            'folder': folder,
            'success': False,
            'processing_time': processing_time,
            'error': str(e)
        }

def quick_parallel_import_all(max_workers: int = 4):
    """å¿«é€Ÿä¸¦è¡ŒåŒ¯å…¥æ‰€æœ‰è³‡æ–™å¤¾"""
    print(f"ğŸš€ å¿«é€Ÿä¸¦è¡Œæ‰¹æ¬¡åŒ¯å…¥ (ä½¿ç”¨ {max_workers} å€‹åŸ·è¡Œç·’)")
    print("=" * 80)
    
    # çµ±è¨ˆè³‡è¨Š
    total_files = 0
    successful_files = 0
    failed_files = 0
    total_records = 0
    start_time = datetime.now()
    processing_times = []
    lock = threading.Lock()
    
    # æƒææ‰€æœ‰æª”æ¡ˆ
    all_files = []
    for folder in DATA_FOLDERS:
        if os.path.exists(folder):
            csv_files = glob.glob(os.path.join(folder, "*.csv"))
            all_files.extend([(file_path, folder) for file_path in csv_files])
            print(f"ğŸ“ {folder}: {len(csv_files)} å€‹CSVæª”æ¡ˆ")
        else:
            print(f"âŒ è³‡æ–™å¤¾ä¸å­˜åœ¨: {folder}")
    
    total_files = len(all_files)
    print(f"\nğŸ“Š ç¸½è¨ˆ: {total_files} å€‹CSVæª”æ¡ˆ")
    
    if total_files == 0:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•CSVæª”æ¡ˆ")
        return
    
    # é–‹å§‹ä¸¦è¡ŒåŒ¯å…¥
    print(f"\nğŸš€ é–‹å§‹ä¸¦è¡ŒåŒ¯å…¥...")
    start_time = datetime.now()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        future_to_file = {
            executor.submit(import_single_file_worker, file_path, folder): (file_path, folder)
            for file_path, folder in all_files
        }
        
        # ä½¿ç”¨é€²åº¦æ¢è¿½è¹¤é€²åº¦
        with tqdm(total=total_files, desc="ä¸¦è¡ŒåŒ¯å…¥é€²åº¦", unit="æª”æ¡ˆ") as pbar:
            for future in as_completed(future_to_file):
                result = future.result()
                filename = result['filename']
                
                # æ›´æ–°çµ±è¨ˆ
                with lock:
                    if result['success']:
                        successful_files += 1
                        logger.info(f"âœ… {filename} åŒ¯å…¥æˆåŠŸ ({result['processing_time']:.2f}s)")
                    else:
                        failed_files += 1
                        logger.error(f"âŒ {filename} åŒ¯å…¥å¤±æ•—: {result['error']}")
                    
                    processing_times.append(result['processing_time'])
                
                pbar.set_postfix({
                    'æˆåŠŸ': successful_files,
                    'å¤±æ•—': failed_files,
                    'å¹³å‡æ™‚é–“': f"{sum(processing_times)/len(processing_times):.2f}s"
                })
                pbar.update(1)
    
    # è¨ˆç®—çµ±è¨ˆ
    end_time = datetime.now()
    duration = end_time - start_time
    success_rate = (successful_files / total_files * 100) if total_files > 0 else 0
    avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
    
    # é¡¯ç¤ºçµæœ
    print(f"\nâœ… ä¸¦è¡Œæ‰¹æ¬¡åŒ¯å…¥å®Œæˆ!")
    print("=" * 80)
    print(f"â±ï¸  ç¸½è€—æ™‚: {duration}")
    print(f"ğŸ§µ ä½¿ç”¨åŸ·è¡Œç·’æ•¸: {max_workers}")
    print(f"ğŸ“ ç¸½æª”æ¡ˆæ•¸: {total_files}")
    print(f"âœ… æˆåŠŸæª”æ¡ˆæ•¸: {successful_files}")
    print(f"âŒ å¤±æ•—æª”æ¡ˆæ•¸: {failed_files}")
    print(f"ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
    print(f"â±ï¸  å¹³å‡è™•ç†æ™‚é–“: {avg_processing_time:.2f}ç§’/æª”æ¡ˆ")
    print(f"ğŸ“ ç¸½è¨˜éŒ„æ•¸: {total_records:,}")
    
    # æ•ˆèƒ½åˆ†æ
    if processing_times:
        min_time = min(processing_times)
        max_time = max(processing_times)
        print(f"\nğŸ“Š è™•ç†æ™‚é–“åˆ†æ:")
        print(f"  æœ€å¿«æª”æ¡ˆ: {min_time:.2f}ç§’")
        print(f"  æœ€æ…¢æª”æ¡ˆ: {max_time:.2f}ç§’")
        print(f"  å¹³å‡æ™‚é–“: {avg_processing_time:.2f}ç§’")
    
    # ä¿å­˜çµ±è¨ˆåˆ°æª”æ¡ˆ
    stats_file = f"quick_parallel_import_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write(f"å¿«é€Ÿä¸¦è¡ŒåŒ¯å…¥çµ±è¨ˆå ±å‘Š\n")
        f.write(f"åŒ¯å…¥æ™‚é–“: {start_time.strftime('%Y-%m-%d %H:%M:%S')} - {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ç¸½è€—æ™‚: {duration}\n")
        f.write(f"ä½¿ç”¨åŸ·è¡Œç·’æ•¸: {max_workers}\n")
        f.write(f"ç¸½æª”æ¡ˆæ•¸: {total_files}\n")
        f.write(f"æˆåŠŸæª”æ¡ˆæ•¸: {successful_files}\n")
        f.write(f"å¤±æ•—æª”æ¡ˆæ•¸: {failed_files}\n")
        f.write(f"æˆåŠŸç‡: {success_rate:.1f}%\n")
        f.write(f"å¹³å‡è™•ç†æ™‚é–“: {avg_processing_time:.2f}ç§’/æª”æ¡ˆ\n")
        f.write(f"ç¸½è¨˜éŒ„æ•¸: {total_records:,}\n")
    
    print(f"ğŸ“„ çµ±è¨ˆå ±å‘Šå·²ä¿å­˜åˆ°: {stats_file}")

def main():
    """ä¸»å‡½æ•¸"""
    print("é¸æ“‡åŸ·è¡Œæ¨¡å¼:")
    print("1. å¿«é€Ÿä¸¦è¡ŒåŒ¯å…¥ (4åŸ·è¡Œç·’)")
    print("2. é«˜ä¸¦è¡ŒåŒ¯å…¥ (8åŸ·è¡Œç·’)")
    print("3. è‡ªè¨‚åŸ·è¡Œç·’æ•¸")
    print("4. åªåŒ¯å…¥ç¬¬ä¸€å€‹è³‡æ–™å¤¾ (æ¸¬è©¦)")
    
    choice = input("è«‹é¸æ“‡ (1/2/3/4): ").strip()
    
    if choice == "1":
        quick_parallel_import_all(max_workers=4)
    elif choice == "2":
        quick_parallel_import_all(max_workers=8)
    elif choice == "3":
        try:
            max_workers = int(input("è«‹è¼¸å…¥åŸ·è¡Œç·’æ•¸ (å»ºè­° 2-8): ").strip())
            quick_parallel_import_all(max_workers=max_workers)
        except ValueError:
            print("âŒ ç„¡æ•ˆçš„åŸ·è¡Œç·’æ•¸ï¼Œä½¿ç”¨é è¨­å€¼ 4")
            quick_parallel_import_all(max_workers=4)
    elif choice == "4":
        # åªåŒ¯å…¥ç¬¬ä¸€å€‹è³‡æ–™å¤¾
        if DATA_FOLDERS and os.path.exists(DATA_FOLDERS[0]):
            folder = DATA_FOLDERS[0]
            csv_files = glob.glob(os.path.join(folder, "*.csv"))
            print(f"ğŸ“ æ¸¬è©¦è³‡æ–™å¤¾: {folder} ({len(csv_files)} å€‹æª”æ¡ˆ)")
            
            # åªè™•ç†å‰10å€‹æª”æ¡ˆ
            test_files = csv_files[:10]
            print(f"ğŸ“„ æ¸¬è©¦æª”æ¡ˆæ•¸: {len(test_files)}")
            
            importer = EnhancedDataImporter()
            successful = 0
            failed = 0
            
            with ThreadPoolExecutor(max_workers=4) as executor:
                future_to_file = {
                    executor.submit(import_single_file_worker, file_path, folder): file_path
                    for file_path in test_files
                }
                
                with tqdm(total=len(test_files), desc="æ¸¬è©¦åŒ¯å…¥", unit="æª”æ¡ˆ") as pbar:
                    for future in as_completed(future_to_file):
                        result = future.result()
                        if result['success']:
                            successful += 1
                            print(f"âœ… {result['filename']}")
                        else:
                            failed += 1
                            print(f"âŒ {result['filename']}: {result['error']}")
                        pbar.update(1)
            
            print(f"\næ¸¬è©¦çµæœ: æˆåŠŸ {successful}, å¤±æ•— {failed}")
        else:
            print("âŒ æ²’æœ‰å¯ç”¨çš„æ¸¬è©¦è³‡æ–™å¤¾")
    else:
        print("âŒ ç„¡æ•ˆçš„é¸æ“‡")

if __name__ == "__main__":
    main()




