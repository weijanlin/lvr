# -*- coding: utf-8 -*-
"""
LVR è³‡æ–™åŒ¯å…¥å™¨
è² è²¬å°‡ CSV æª”æ¡ˆåŒ¯å…¥åˆ°å°æ‡‰çš„è³‡æ–™åº«ä¸­
"""

import os
import pandas as pd
import pyodbc
import logging
from typing import Dict, List, Optional, Tuple
from tqdm import tqdm
import time

from config import DB_CONFIG, DATABASES, DATA_FOLDERS, BATCH_SIZE

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('lvr_import.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class DataImporter:
    """è³‡æ–™åŒ¯å…¥å™¨é¡åˆ¥"""
    
    def __init__(self):
        self.connection_string = self._build_connection_string()
        
    def _build_connection_string(self) -> str:
        """å»ºç«‹è³‡æ–™åº«é€£ç·šå­—ä¸²"""
        conn_str = (
            f"DRIVER={{{DB_CONFIG['driver']}}};"
            f"SERVER={DB_CONFIG['server']};"
            f"UID={DB_CONFIG['username']};"
            f"PWD={DB_CONFIG['password']};"
            f"Trusted_Connection={DB_CONFIG['trusted_connection']};"
            f"Encrypt={DB_CONFIG['encrypt']};"
        )
        return conn_str
    
    def get_database_for_file(self, filename: str) -> str:
        """æ ¹æ“šæª”æ¡ˆåç¨±åˆ¤æ–·æ‡‰è©²åŒ¯å…¥åˆ°å“ªå€‹è³‡æ–™åº«"""
        if '_a.csv' in filename:
            return DATABASES['used_house']      # ä¸­å¤å±‹è²·è³£
        elif '_b.csv' in filename:
            return DATABASES['pre_sale']        # é å”®å±‹è²·è³£
        elif '_c.csv' in filename:
            return DATABASES['rental']          # ç§Ÿå±‹
        else:
            return None
    
    def get_table_for_file(self, filename: str) -> str:
        """æ ¹æ“šæª”æ¡ˆåç¨±åˆ¤æ–·æ‡‰è©²åŒ¯å…¥åˆ°å“ªå€‹è³‡æ–™è¡¨"""
        if '_a.csv' in filename:
            return 'main_data'
        elif '_a_build.csv' in filename:
            return 'build_data'
        elif '_a_land.csv' in filename:
            return 'land_data'
        elif '_a_park.csv' in filename:
            return 'park_data'
        elif '_b.csv' in filename:
            return 'main_data'
        elif '_b_build.csv' in filename:
            return 'build_data'
        elif '_b_land.csv' in filename:
            return 'land_data'
        elif '_b_park.csv' in filename:
            return 'park_data'
        elif '_c.csv' in filename:
            return 'main_data'
        elif '_c_build.csv' in filename:
            return 'build_data'
        elif '_c_land.csv' in filename:
            return 'land_data'
        elif '_c_park.csv' in filename:
            return 'park_data'
        else:
            return 'main_data'
    
    def read_csv_file(self, file_path: str) -> Optional[pd.DataFrame]:
        """è®€å– CSV æª”æ¡ˆ"""
        try:
            # å˜—è©¦ä¸åŒçš„ç·¨ç¢¼
            encodings = ['utf-8', 'big5', 'cp950', 'gbk']
            
            for encoding in encodings:
                try:
                    # è·³éç¬¬äºŒè¡Œï¼ˆæ¬„ä½åç¨±è¡Œï¼‰
                    df = pd.read_csv(file_path, encoding=encoding, skiprows=[1])
                    logger.info(f"âœ… æˆåŠŸè®€å– {file_path} (ç·¨ç¢¼: {encoding})")
                    return df
                except UnicodeDecodeError:
                    continue
            
            logger.error(f"âŒ ç„¡æ³•è®€å– {file_path}ï¼Œæ‰€æœ‰ç·¨ç¢¼éƒ½å¤±æ•—")
            return None
            
        except Exception as e:
            logger.error(f"âŒ è®€å– {file_path} å¤±æ•—: {str(e)}")
            return None
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†è³‡æ–™"""
        try:
            # ç§»é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
            df = df.dropna(how='all')
            
            # å®šç¾©æ•¸å€¼æ¬„ä½åç¨±ï¼ˆæ ¹æ“šè³‡æ–™è¡¨çµæ§‹ï¼‰
            numeric_columns = [
                'åœŸåœ°ç§»è½‰ç¸½é¢ç©å¹³æ–¹å…¬å°º', 'äº¤æ˜“ç­†æ£Ÿæ•¸', 'ç¸½æ¨“å±¤æ•¸', 'å»ºç‰©ç§»è½‰ç¸½é¢ç©å¹³æ–¹å…¬å°º',
                'å»ºç‰©ç¾æ³æ ¼å±€-æˆ¿', 'å»ºç‰©ç¾æ³æ ¼å±€-å»³', 'å»ºç‰©ç¾æ³æ ¼å±€-è¡›', 'ç¸½åƒ¹å…ƒ', 'å–®åƒ¹å…ƒå¹³æ–¹å…¬å°º',
                'è»Šä½ç§»è½‰ç¸½é¢ç©å¹³æ–¹å…¬å°º', 'è»Šä½ç¸½åƒ¹å…ƒ', 'ä¸»å»ºç‰©é¢ç©', 'é™„å±¬å»ºç‰©é¢ç©', 'é™½å°é¢ç©',
                'å±‹é½¡', 'å»ºç‰©ç§»è½‰é¢ç©å¹³æ–¹å…¬å°º', 'åœŸåœ°ç§»è½‰é¢ç©å¹³æ–¹å…¬å°º', 'æ¬Šåˆ©äººæŒåˆ†åˆ†æ¯', 'æ¬Šåˆ©äººæŒåˆ†åˆ†å­',
                'è»Šä½åƒ¹æ ¼', 'è»Šä½é¢ç©å¹³æ–¹å…¬å°º', 'åœŸåœ°é¢ç©å¹³æ–¹å…¬å°º', 'ç§Ÿè³ƒç­†æ£Ÿæ•¸', 'å»ºç‰©ç¸½é¢ç©å¹³æ–¹å…¬å°º',
                'è»Šä½é¢ç©å¹³æ–¹å…¬å°º', 'è»Šä½ç¸½é¡å…ƒ', 'ç¸½é¡å…ƒ'
            ]
            
            # è™•ç†æ•¸å€¼æ¬„ä½
            for col in numeric_columns:
                if col in df.columns:
                    # å…ˆè½‰æ›ç‚ºå­—ä¸²ï¼Œç„¶å¾Œæ¸…ç†
                    df[col] = df[col].astype(str)
                    # ç§»é™¤éæ•¸å€¼å­—ç¬¦ï¼ˆä¿ç•™å°æ•¸é»å’Œè² è™Ÿï¼‰
                    df[col] = df[col].str.replace(r'[^\d.-]', '', regex=True)
                    # è™•ç†ç©ºå­—ä¸²å’Œç„¡æ•ˆå€¼
                    df[col] = df[col].replace(['', 'nan', 'None', 'null'], None)
                    # è½‰æ›ç‚ºæ•¸å€¼ï¼Œç„¡æ³•è½‰æ›çš„è¨­ç‚º None
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                    # å°‡ NaN è½‰æ›ç‚º None
                    df[col] = df[col].where(pd.notnull(df[col]), None)
            
            # è™•ç†å­—ä¸²æ¬„ä½çš„ç©ºå€¼
            string_columns = df.select_dtypes(include=['object']).columns
            for col in string_columns:
                df[col] = df[col].fillna('')
                # æ¸…ç†å­—ä¸²ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                df[col] = df[col].astype(str).str.replace('\r\n', ' ').str.replace('\n', ' ').str.strip()
                # è™•ç† 'nan' å­—ä¸²
                df[col] = df[col].replace('nan', '')
            
            logger.info(f"âœ… è³‡æ–™æ¸…ç†å®Œæˆï¼Œå‰©é¤˜ {len(df)} è¡Œ")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™æ¸…ç†å¤±æ•—: {str(e)}")
            return df
    
    def create_insert_sql(self, table_name: str, columns: List[str]) -> str:
        """å»ºç«‹ INSERT SQL èªå¥"""
        # åŠ å…¥é¡å¤–çš„æ¬„ä½
        all_columns = columns + ['source_file', 'quarter']
        placeholders = ', '.join(['?' for _ in all_columns])
        
        sql = f"""
        INSERT INTO [{table_name}] 
        ([{'], ['.join(all_columns)}])
        VALUES ({placeholders})
        """
        return sql
    
    def insert_data_batch(self, database_name: str, table_name: str, df: pd.DataFrame, 
                         source_file: str, quarter: str) -> bool:
        """æ‰¹æ¬¡æ’å…¥è³‡æ–™"""
        try:
            # é€£æ¥åˆ°æŒ‡å®šè³‡æ–™åº«
            conn_str = self.connection_string + f"Database={database_name};"
            conn = pyodbc.connect(conn_str)
            cursor = conn.cursor()
            
            # æº–å‚™è³‡æ–™
            columns = list(df.columns)
            insert_sql = self.create_insert_sql(table_name, columns)
            
            # æ‰¹æ¬¡è™•ç†
            total_rows = len(df)
            success_count = 0
            
            for i in range(0, total_rows, BATCH_SIZE):
                batch_df = df.iloc[i:i+BATCH_SIZE]
                batch_data = []
                
                for _, row in batch_df.iterrows():
                    # æº–å‚™è³‡æ–™è¡Œï¼Œè™•ç†è³‡æ–™é¡å‹
                    row_data = []
                    for value in row.values:
                        if pd.isna(value) or value is None:
                            row_data.append(None)
                        elif isinstance(value, (int, float)):
                            # ç¢ºä¿æ•¸å€¼åœ¨åˆç†ç¯„åœå…§
                            if isinstance(value, float) and (value > 1e15 or value < -1e15):
                                row_data.append(None)
                            else:
                                row_data.append(value)
                        else:
                            # å­—ä¸²è³‡æ–™
                            str_value = str(value).strip()
                            if str_value in ['', 'nan', 'None', 'null']:
                                row_data.append(None)
                            else:
                                row_data.append(str_value)
                    
                    # åŠ å…¥é¡å¤–æ¬„ä½
                    row_data.extend([source_file, quarter])
                    batch_data.append(row_data)
                
                # åŸ·è¡Œæ‰¹æ¬¡æ’å…¥
                cursor.executemany(insert_sql, batch_data)
                success_count += len(batch_data)
                
                # é¡¯ç¤ºé€²åº¦
                progress = min(i + BATCH_SIZE, total_rows)
                logger.info(f"ğŸ“Š é€²åº¦: {progress}/{total_rows} è¡Œå·²è™•ç†")
            
            conn.commit()
            conn.close()
            
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {success_count} è¡Œåˆ° {database_name}.{table_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ’å…¥è³‡æ–™åˆ° {database_name}.{table_name} å¤±æ•—: {str(e)}")
            return False
    
    def import_single_folder(self, folder_path: str) -> Dict[str, int]:
        """åŒ¯å…¥å–®ä¸€è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ CSV æª”æ¡ˆ"""
        logger.info(f"ğŸš€ é–‹å§‹åŒ¯å…¥è³‡æ–™å¤¾: {folder_path}")
        
        # è®€å– manifest.csv
        manifest_path = os.path.join(folder_path, 'manifest.csv')
        if not os.path.exists(manifest_path):
            logger.error(f"âŒ manifest.csv ä¸å­˜åœ¨: {manifest_path}")
            return {}
        
        try:
            manifest_df = pd.read_csv(manifest_path, encoding='utf-8')
            logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(manifest_df)} å€‹æª”æ¡ˆè¨˜éŒ„")
        except Exception as e:
            logger.error(f"âŒ è®€å– manifest.csv å¤±æ•—: {str(e)}")
            return {}
        
        # çµ±è¨ˆçµæœ
        import_stats = {
            'total_files': 0,
            'success_files': 0,
            'failed_files': 0,
            'total_rows': 0
        }
        
        # è™•ç†æ¯å€‹ CSV æª”æ¡ˆ
        for _, row in manifest_df.iterrows():
            filename = row['name']
            if not filename.endswith('.csv'):
                continue
            
            file_path = os.path.join(folder_path, filename)
            if not os.path.exists(file_path):
                logger.warning(f"âš ï¸ æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
                continue
            
            import_stats['total_files'] += 1
            
            try:
                # åˆ¤æ–·ç›®æ¨™è³‡æ–™åº«å’Œè³‡æ–™è¡¨
                database_name = self.get_database_for_file(filename)
                table_name = self.get_table_for_file(filename)
                
                if not database_name:
                    logger.warning(f"âš ï¸ ç„¡æ³•åˆ¤æ–·æª”æ¡ˆé¡å‹: {filename}")
                    import_stats['failed_files'] += 1
                    continue
                
                logger.info(f"ğŸ“ è™•ç†æª”æ¡ˆ: {filename} -> {database_name}.{table_name}")
                
                # è®€å– CSV æª”æ¡ˆ
                df = self.read_csv_file(file_path)
                if df is None or len(df) == 0:
                    logger.warning(f"âš ï¸ æª”æ¡ˆç‚ºç©ºæˆ–è®€å–å¤±æ•—: {filename}")
                    import_stats['failed_files'] += 1
                    continue
                
                # æ¸…ç†è³‡æ–™
                df = self.clean_data(df)
                
                # æ’å…¥è³‡æ–™
                quarter = folder_path  # ä½¿ç”¨è³‡æ–™å¤¾åç¨±ä½œç‚ºå­£åº¦æ¨™è­˜
                if self.insert_data_batch(database_name, table_name, df, filename, quarter):
                    import_stats['success_files'] += 1
                    import_stats['total_rows'] += len(df)
                    logger.info(f"âœ… æˆåŠŸåŒ¯å…¥ {filename}: {len(df)} è¡Œ")
                else:
                    import_stats['failed_files'] += 1
                    logger.error(f"âŒ åŒ¯å…¥å¤±æ•—: {filename}")
                
            except Exception as e:
                logger.error(f"âŒ è™•ç†æª”æ¡ˆ {filename} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                import_stats['failed_files'] += 1
        
        # é¡¯ç¤ºåŒ¯å…¥çµ±è¨ˆ
        logger.info(f"ğŸ“Š è³‡æ–™å¤¾ {folder_path} åŒ¯å…¥å®Œæˆ:")
        logger.info(f"   ç¸½æª”æ¡ˆæ•¸: {import_stats['total_files']}")
        logger.info(f"   æˆåŠŸæª”æ¡ˆæ•¸: {import_stats['success_files']}")
        logger.info(f"   å¤±æ•—æª”æ¡ˆæ•¸: {import_stats['failed_files']}")
        logger.info(f"   ç¸½è³‡æ–™è¡Œæ•¸: {import_stats['total_rows']}")
        
        return import_stats
    
    def import_all_folders(self) -> Dict[str, Dict[str, int]]:
        """åŒ¯å…¥æ‰€æœ‰è³‡æ–™å¤¾"""
        logger.info("ğŸš€ é–‹å§‹åŒ¯å…¥æ‰€æœ‰è³‡æ–™å¤¾")
        
        all_stats = {}
        
        for folder in DATA_FOLDERS:
            if os.path.exists(folder):
                logger.info(f"ğŸ“ è™•ç†è³‡æ–™å¤¾: {folder}")
                stats = self.import_single_folder(folder)
                all_stats[folder] = stats
                
                # æš«åœä¸€ä¸‹ï¼Œé¿å…éåº¦æ¶ˆè€—è³‡æº
                time.sleep(1)
            else:
                logger.warning(f"âš ï¸ è³‡æ–™å¤¾ä¸å­˜åœ¨: {folder}")
        
        # é¡¯ç¤ºç¸½é«”çµ±è¨ˆ
        total_files = sum(stats['total_files'] for stats in all_stats.values())
        total_success = sum(stats['success_files'] for stats in all_stats.values())
        total_failed = sum(stats['failed_files'] for stats in all_stats.values())
        total_rows = sum(stats['total_rows'] for stats in all_stats.values())
        
        logger.info("ğŸ‰ æ‰€æœ‰è³‡æ–™å¤¾åŒ¯å…¥å®Œæˆï¼")
        logger.info(f"ğŸ“Š ç¸½é«”çµ±è¨ˆ:")
        logger.info(f"   ç¸½æª”æ¡ˆæ•¸: {total_files}")
        logger.info(f"   æˆåŠŸæª”æ¡ˆæ•¸: {total_success}")
        logger.info(f"   å¤±æ•—æª”æ¡ˆæ•¸: {total_failed}")
        logger.info(f"   ç¸½è³‡æ–™è¡Œæ•¸: {total_rows}")
        
        return all_stats


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ LVR è³‡æ–™åŒ¯å…¥å·¥å…·")
    print("=" * 60)
    
    # å»ºç«‹è³‡æ–™åŒ¯å…¥å™¨
    importer = DataImporter()
    
    # è©¢å•ä½¿ç”¨è€…è¦åŒ¯å…¥å“ªå€‹è³‡æ–™å¤¾
    print("\nğŸ“ å¯ç”¨çš„è³‡æ–™å¤¾:")
    for i, folder in enumerate(DATA_FOLDERS, 1):
        if os.path.exists(folder):
            print(f"   {i}. {folder}")
    
    print("\nè«‹é¸æ“‡:")
    print("1. åŒ¯å…¥å–®ä¸€è³‡æ–™å¤¾")
    print("2. åŒ¯å…¥æ‰€æœ‰è³‡æ–™å¤¾")
    print("3. é€€å‡º")
    
    choice = input("\nè«‹è¼¸å…¥é¸æ“‡ (1-3): ").strip()
    
    if choice == '1':
        # åŒ¯å…¥å–®ä¸€è³‡æ–™å¤¾
        print("\nè«‹é¸æ“‡è¦åŒ¯å…¥çš„è³‡æ–™å¤¾:")
        for i, folder in enumerate(DATA_FOLDERS, 1):
            if os.path.exists(folder):
                print(f"   {i}. {folder}")
        
        folder_choice = input(f"\nè«‹è¼¸å…¥é¸æ“‡ (1-{len(DATA_FOLDERS)}): ").strip()
        try:
            folder_index = int(folder_choice) - 1
            if 0 <= folder_index < len(DATA_FOLDERS):
                folder = DATA_FOLDERS[folder_index]
                if os.path.exists(folder):
                    print(f"\nğŸš€ é–‹å§‹åŒ¯å…¥è³‡æ–™å¤¾: {folder}")
                    stats = importer.import_single_folder(folder)
                    print(f"\nâœ… åŒ¯å…¥å®Œæˆï¼")
                    print(f"ğŸ“Š çµ±è¨ˆ: {stats}")
                else:
                    print(f"âŒ è³‡æ–™å¤¾ä¸å­˜åœ¨: {folder}")
            else:
                print("âŒ ç„¡æ•ˆçš„é¸æ“‡")
        except ValueError:
            print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—")
    
    elif choice == '2':
        # åŒ¯å…¥æ‰€æœ‰è³‡æ–™å¤¾
        print("\nğŸš€ é–‹å§‹åŒ¯å…¥æ‰€æœ‰è³‡æ–™å¤¾...")
        all_stats = importer.import_all_folders()
        print(f"\nâœ… æ‰€æœ‰è³‡æ–™å¤¾åŒ¯å…¥å®Œæˆï¼")
        print(f"ğŸ“Š ç¸½é«”çµ±è¨ˆ:")
        for folder, stats in all_stats.items():
            print(f"   {folder}: {stats['success_files']}/{stats['total_files']} æª”æ¡ˆæˆåŠŸ")
    
    elif choice == '3':
        print("ğŸ‘‹ å†è¦‹ï¼")
    
    else:
        print("âŒ ç„¡æ•ˆçš„é¸æ“‡")


if __name__ == "__main__":
    main()
